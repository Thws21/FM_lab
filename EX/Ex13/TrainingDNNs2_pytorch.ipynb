{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Deep Neural Networks (Tiếp)\n",
        "Trong buổi này, chúng ta tiếp tục làm quen với một số kỹ thuật huấn luyện mạng nơ-ron"
      ],
      "metadata": {
        "id": "9dW-dnoOkkAp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "!nvidia-smi\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov  1 15:25:47 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dALhhAr_5agu",
        "outputId": "221c5fee-01e1-4555-bffa-d6914a039e8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Cài đặt BatchNorm"
      ],
      "metadata": {
        "id": "Doqa-Juc4rls"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "def compare_bn(bn1, bn2):\n",
        "    err = False\n",
        "    if not torch.allclose(bn1.running_mean, bn2.running_mean):\n",
        "        print('Diff in running_mean: {} vs {}'.format(\n",
        "            bn1.running_mean, bn2.running_mean))\n",
        "        err = True\n",
        "\n",
        "    if not torch.allclose(bn1.running_var, bn2.running_var):\n",
        "        print('Diff in running_var: {} vs {}'.format(\n",
        "            bn1.running_var, bn2.running_var))\n",
        "        err = True\n",
        "\n",
        "    if bn1.affine and bn2.affine:\n",
        "        if not torch.allclose(bn1.weight, bn2.weight):\n",
        "            print('Diff in weight: {} vs {}'.format(\n",
        "                bn1.weight, bn2.weight))\n",
        "            err = True\n",
        "\n",
        "        if not torch.allclose(bn1.bias, bn2.bias):\n",
        "            print('Diff in bias: {} vs {}'.format(\n",
        "                bn1.bias, bn2.bias))\n",
        "            err = True\n",
        "\n",
        "    if not err:\n",
        "        print('All parameters are equal!')\n",
        "\n",
        "\n",
        "class MyBatchNorm2d(nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1,\n",
        "                 affine=True, track_running_stats=True):\n",
        "        super(MyBatchNorm2d, self).__init__(\n",
        "            num_features, eps, momentum, affine, track_running_stats)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self._check_input_dim(input)\n",
        "\n",
        "        exponential_average_factor = 0.0\n",
        "\n",
        "        if self.training and self.track_running_stats:\n",
        "            if self.num_batches_tracked is not None:\n",
        "                self.num_batches_tracked += 1\n",
        "                if self.momentum is None:  # use cumulative moving average\n",
        "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
        "                else:  # use exponential moving average\n",
        "                    exponential_average_factor = self.momentum\n",
        "\n",
        "        # calculate running estimates\n",
        "        if self.training:\n",
        "            mean = input.mean([0, 2, 3])\n",
        "            # use biased var in train\n",
        "            var = input.var([0, 2, 3], unbiased=False)\n",
        "            n = input.numel() / input.size(1)\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = exponential_average_factor * mean\\\n",
        "                    + (1 - exponential_average_factor) * self.running_mean\n",
        "                # update running_var with unbiased var\n",
        "                self.running_var = exponential_average_factor * var * n / (n - 1)\\\n",
        "                    + (1 - exponential_average_factor) * self.running_var\n",
        "        else:\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        # Chuẩn hoá input\n",
        "        ######################\n",
        "        ### YOUR CODE HERE ###\n",
        "        input = (input - mean[None, :, None, None]) / (torch.sqrt(var[None, :, None, None] + self.eps))\n",
        "        ######################\n",
        "        if self.affine:\n",
        "            # Biến đổi input sau khi chuẩn hoá\n",
        "            ######################\n",
        "            ### YOUR CODE HERE ###\n",
        "            input = input * self.weight[None, :, None, None] + self.bias[None, :, None, None]\n",
        "            ######################\n",
        "\n",
        "        return input\n",
        "\n",
        "\n",
        "# Init BatchNorm layers\n",
        "my_bn = MyBatchNorm2d(3, affine=True)\n",
        "bn = nn.BatchNorm2d(3, affine=True)\n",
        "\n",
        "compare_bn(my_bn, bn)  # weight and bias should be different\n",
        "# Load weight and bias\n",
        "my_bn.load_state_dict(bn.state_dict())\n",
        "compare_bn(my_bn, bn)\n",
        "\n",
        "# Run train\n",
        "for _ in range(10):\n",
        "    scale = torch.randint(1, 10, (1,)).float()\n",
        "    bias = torch.randint(-10, 10, (1,)).float()\n",
        "    x = torch.randn(10, 3, 100, 100) * scale + bias\n",
        "    out1 = my_bn(x)\n",
        "    out2 = bn(x)\n",
        "    compare_bn(my_bn, bn)\n",
        "\n",
        "    torch.allclose(out1, out2)\n",
        "    print('Max diff: ', (out1 - out2).abs().max())\n",
        "\n",
        "# Run eval\n",
        "my_bn.eval()\n",
        "bn.eval()\n",
        "for _ in range(10):\n",
        "    scale = torch.randint(1, 10, (1,)).float()\n",
        "    bias = torch.randint(-10, 10, (1,)).float()\n",
        "    x = torch.randn(10, 3, 100, 100) * scale + bias\n",
        "    out1 = my_bn(x)\n",
        "    out2 = bn(x)\n",
        "    compare_bn(my_bn, bn)\n",
        "\n",
        "    torch.allclose(out1, out2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All parameters are equal!\n",
            "All parameters are equal!\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(1.4305e-06, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(7.1526e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "All parameters are equal!\n",
            "All parameters are equal!\n",
            "All parameters are equal!\n",
            "All parameters are equal!\n",
            "All parameters are equal!\n",
            "All parameters are equal!\n",
            "All parameters are equal!\n",
            "All parameters are equal!\n",
            "All parameters are equal!\n"
          ]
        }
      ],
      "metadata": {
        "id": "1PdGnjmyOM47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee969b2-6103-4b93-e9ba-5175fb7c6b40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Cài đặt chiến lược thay đổi tốc độ học: Warm-up + Cosine Annealing LR"
      ],
      "metadata": {
        "id": "njZw8LOwCrsg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "def load_data(data_dir=\"./data\"):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=transform)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=transform)\n",
        "\n",
        "    return trainset, testset\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, l1=120, l2=84):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
        "        self.fc2 = nn.Linear(l1, l2)\n",
        "        self.fc3 = nn.Linear(l2, 10)\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ],
      "outputs": [],
      "metadata": {
        "id": "ptNpnOaYCuK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "trainset, testset = load_data('./data')\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "epochs = 50\n",
        "warm_epoch = 5\n",
        "init_lr = 1e-1\n",
        "last_lr = 1e-5\n",
        "T_max = epochs\n",
        "T_cur = 0\n",
        "lr_list = [0]\n",
        "\n",
        "net = Net()\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda:0\"\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        net = nn.DataParallel(net)\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=init_lr, momentum=0.9)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.8MB/s]\n"
          ]
        }
      ],
      "metadata": {
        "id": "mS4soUx9iwvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1aa5a1-99da-44c1-d44a-6d9aa517c94a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "for epoch in range(1, epochs+1):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    epoch_steps = 0\n",
        "    T_cur += 1\n",
        "\n",
        "    # warm-up and cosine annealing lr\n",
        "\n",
        "    ######################\n",
        "    ### YOUR CODE HERE ###\n",
        "    if epoch <= warm_epoch:\n",
        "        optimizer.param_groups[0]['lr'] = (1.0 * epoch) / warm_epoch  * init_lr\n",
        "    else:\n",
        "        # cosine annealing lr\n",
        "        optimizer.param_groups[0]['lr'] = last_lr + (init_lr - last_lr) * (1 + np.cos(T_cur * np.pi / T_max)) / 2\n",
        "\n",
        "    ######################\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        epoch_steps += 1\n",
        "        if i + 1 == len(trainloader):\n",
        "            print(\"[Epoch %d] loss: %.3f\" % (epoch, running_loss / epoch_steps))\n",
        "            running_loss = 0.0\n",
        "\n",
        "    lr_list.append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "print(\"Finished Training\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] loss: 2.301\n",
            "[Epoch 2] loss: 2.196\n",
            "[Epoch 3] loss: 2.099\n",
            "[Epoch 4] loss: 2.108\n",
            "[Epoch 5] loss: 2.248\n",
            "[Epoch 6] loss: 2.313\n",
            "[Epoch 7] loss: 2.324\n",
            "[Epoch 8] loss: 2.320\n",
            "[Epoch 9] loss: 2.339\n",
            "[Epoch 10] loss: 2.361\n",
            "[Epoch 11] loss: 2.361\n",
            "[Epoch 12] loss: 2.361\n",
            "[Epoch 13] loss: 2.361\n",
            "[Epoch 14] loss: 2.361\n",
            "[Epoch 15] loss: 2.361\n",
            "[Epoch 16] loss: 2.361\n",
            "[Epoch 17] loss: 2.361\n",
            "[Epoch 18] loss: 2.361\n",
            "[Epoch 19] loss: 2.361\n",
            "[Epoch 20] loss: 2.361\n",
            "[Epoch 21] loss: 2.361\n",
            "[Epoch 22] loss: 2.361\n",
            "[Epoch 23] loss: 2.361\n",
            "[Epoch 24] loss: 2.361\n",
            "[Epoch 25] loss: 2.361\n",
            "[Epoch 26] loss: 2.361\n",
            "[Epoch 27] loss: 2.361\n",
            "[Epoch 28] loss: 2.361\n",
            "[Epoch 29] loss: 2.361\n",
            "[Epoch 30] loss: 2.361\n",
            "[Epoch 31] loss: 2.361\n",
            "[Epoch 32] loss: 2.361\n",
            "[Epoch 33] loss: 2.361\n",
            "[Epoch 34] loss: 2.361\n",
            "[Epoch 35] loss: 2.361\n",
            "[Epoch 36] loss: 2.361\n",
            "[Epoch 37] loss: 2.361\n",
            "[Epoch 38] loss: 2.361\n",
            "[Epoch 39] loss: 2.361\n",
            "[Epoch 40] loss: 2.361\n",
            "[Epoch 41] loss: 2.361\n",
            "[Epoch 42] loss: 2.361\n",
            "[Epoch 43] loss: 2.361\n",
            "[Epoch 44] loss: 2.361\n",
            "[Epoch 45] loss: 2.361\n",
            "[Epoch 46] loss: 2.361\n",
            "[Epoch 47] loss: 2.361\n",
            "[Epoch 48] loss: 2.361\n",
            "[Epoch 49] loss: 2.361\n",
            "[Epoch 50] loss: 2.361\n",
            "Finished Training\n"
          ]
        }
      ],
      "metadata": {
        "id": "ubn7Gcqg0tAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894f0910-e0b1-4fd0-c3bd-97c30f53088e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(list(range(len(lr_list))), lr_list, label=\"lr\")\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.ylabel(\"Learning rate\", fontsize=14)\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "8T-wth2107T6",
        "outputId": "2c4bf97f-aeb1-426b-a42f-a8ea1a579168"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Tuning siêu tham số"
      ],
      "metadata": {
        "id": "lumzKlKrCuj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cài đặt thư viện ray"
      ],
      "metadata": {
        "id": "RxjWIybgBZ81"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "!pip install ray"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray\n",
            "  Downloading ray-2.51.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting click!=8.3.0,>=7.0 (from ray)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray) (3.20.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray) (4.25.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ray) (25.0)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from ray) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ray) (2.32.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (0.28.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema->ray) (4.15.0)\n",
            "Downloading ray-2.51.1-cp312-cp312-manylinux2014_x86_64.whl (71.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: click, ray\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "Successfully installed click-8.2.1 ray-2.51.1\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_SIswTeFf8s",
        "outputId": "8edaa513-0790-4c0e-e999-4add9feb1199"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "import os\n",
        "from ray.tune import CLIReporter\n",
        "from ray import tune\n",
        "from torch.utils.data import random_split\n",
        "from functools import partial"
      ],
      "outputs": [],
      "metadata": {
        "id": "-eA-AZqOJc21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viết hàm huấn luyện mô hình nơ-ron trên tập CIFAR10"
      ],
      "metadata": {
        "id": "1N2ZiY7FBZ82"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "def train_cifar(config, checkpoint_dir=None, data_dir=None):\n",
        "    net = Net(config[\"l1\"], config[\"l2\"])\n",
        "\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            net = nn.DataParallel(net)\n",
        "    net.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        net.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "    trainset, testset = load_data(data_dir)\n",
        "\n",
        "    test_abs = int(len(trainset) * 0.8)\n",
        "    train_subset, val_subset = random_split(\n",
        "        trainset, [test_abs, len(trainset) - test_abs])\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "\n",
        "    for epoch in range(5):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        epoch_steps = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
        "                                                running_loss / epoch_steps))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Validation loss\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(valloader, 0):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.cpu().numpy()\n",
        "                val_steps += 1\n",
        "\n",
        "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "            print(\"*\"*10, path, \"*\"*10)\n",
        "\n",
        "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
        "    print(\"Finished Training\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "rwIo-TBJHRHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lựa chọn siêu tham số tốt (hyperparameter tuning): l1 nhận giá trị 32 hoặc 64, l2 nhận giá trị 32 hoặc 64, lr bằng 1e-4 hoặc 1e-2, batchsize cố định 16"
      ],
      "metadata": {
        "id": "DWk1Q18bBZ82"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "def main(num_samples=10, gpus_per_trial=2):\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "\n",
        "    # Liệt kê các giá trị của các siêu tham số cần tìm kiếm\n",
        "    ######################\n",
        "    ### YOUR CODE HERE ###\n",
        "    config = {\n",
        "        \"l1\": tune.grid_search([32, 64]),\n",
        "        \"l2\": tune.grid_search([16, 32]),\n",
        "        \"lr\": tune.grid_search([1e-4, 1e-2]),\n",
        "        \"batch_size\": tune.grid_search([16])\n",
        "    }\n",
        "    ######################\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "    result = tune.run(\n",
        "        partial(train_cifar, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "main(num_samples=10, gpus_per_trial=1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:40:27,644\tINFO worker.py:2012 -- Started a local Ray instance.\n",
            "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
            "  warnings.warn(\n",
            "2025-11-01 15:40:36,419\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
            "2025-11-01 15:40:36,465\tINFO tensorboardx.py:193 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
            "2025-11-01 15:40:36,469\tWARNING callback.py:143 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n",
            "2025-11-01 15:40:36,473\tWARNING tune.py:902 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------+\n",
            "| Configuration for experiment     train_cifar_2025-11-01_15-40-36   |\n",
            "+--------------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator             |\n",
            "| Scheduler                        FIFOScheduler                     |\n",
            "| Number of trials                 80                                |\n",
            "+--------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/train_cifar_2025-11-01_15-40-36\n",
            "\n",
            "Trial status: 80 PENDING\n",
            "Current time: 2025-11-01 15:40:37. Total running time: 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00000   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   PENDING      32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "75 more PENDING\n",
            "\n",
            "Trial train_cifar_1c521_00000 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00000 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=4297)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=4297)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=4297)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=4297)\u001b[0m   return self._call_impl(*args, **kwargs)\n",
            "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-01 15:40:53,725 E 3991 3991] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[33m(raylet)\u001b[0m [2025-11-01 15:40:57,589 E 4081 4081] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=4297)\u001b[0m [1,  2000] loss: 2.303\n",
            "\n",
            "Trial status: 1 RUNNING | 79 PENDING\n",
            "Current time: 2025-11-01 15:41:07. Total running time: 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00000   RUNNING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00005   PENDING      64     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "74 more PENDING\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:41:09,315\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00000\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=4297, ip=172.28.0.12, actor_id=fb1540963fb69fc4815a654001000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00000 errored after 0 iterations at 2025-11-01 15:41:09. Total running time: 32s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00000_0_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00001 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00001 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=4603)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=4603)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=4603)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=4603)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=4603)\u001b[0m [1,  2000] loss: 2.303\n",
            "\n",
            "Trial status: 1 ERROR | 1 RUNNING | 78 PENDING\n",
            "Current time: 2025-11-01 15:41:37. Total running time: 1min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00001   RUNNING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00005   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00006   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "73 more PENDING\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:41:44,796\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00001\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=4603, ip=172.28.0.12, actor_id=887a5d06aa71da08f08291db01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00001 errored after 0 iterations at 2025-11-01 15:41:44. Total running time: 1min 8s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00001_1_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00002 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00002 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=4893)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=4893)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=4893)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=4893)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 2 ERROR | 1 RUNNING | 77 PENDING\n",
            "Current time: 2025-11-01 15:42:07. Total running time: 1min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00002   RUNNING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00005   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00006   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00007   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "72 more PENDING\n",
            "\u001b[36m(func pid=4893)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:42:18,993\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00002\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=4893, ip=172.28.0.12, actor_id=05c0dfa122e56d41d281316d01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00002 errored after 0 iterations at 2025-11-01 15:42:19. Total running time: 1min 42s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00002_2_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00003 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00003 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=5189)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=5189)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=5189)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=5189)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 3 ERROR | 1 RUNNING | 76 PENDING\n",
            "Current time: 2025-11-01 15:42:37. Total running time: 2min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00003   RUNNING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00005   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00006   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00007   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00008   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "71 more PENDING\n",
            "\u001b[36m(func pid=5189)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:42:53,811\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00003\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=5189, ip=172.28.0.12, actor_id=472bc7cc2aec718d6019e75801000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00003 errored after 0 iterations at 2025-11-01 15:42:53. Total running time: 2min 17s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00003_3_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00004 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00004 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=5476)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=5476)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=5476)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=5476)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 4 ERROR | 1 RUNNING | 75 PENDING\n",
            "Current time: 2025-11-01 15:43:07. Total running time: 2min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00004   RUNNING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00005   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00006   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00007   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00008   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00009   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "70 more PENDING\n",
            "\u001b[36m(func pid=5476)\u001b[0m [1,  2000] loss: 2.266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:43:28,495\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00004\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=5476, ip=172.28.0.12, actor_id=e373d8f12588be844b358d0401000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00004 errored after 0 iterations at 2025-11-01 15:43:28. Total running time: 2min 52s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00004_4_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial status: 5 ERROR | 75 PENDING\n",
            "Current time: 2025-11-01 15:43:37. Total running time: 3min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00005   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00006   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00007   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00008   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00009   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "70 more PENDING\n",
            "\n",
            "Trial train_cifar_1c521_00005 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00005 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=5765)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=5765)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=5765)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=5765)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=5765)\u001b[0m [1,  2000] loss: 2.288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:44:01,965\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00005\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=5765, ip=172.28.0.12, actor_id=fac8bcd0ac06b2bf6d5b323101000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00005 errored after 0 iterations at 2025-11-01 15:44:01. Total running time: 3min 25s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00005_5_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial status: 6 ERROR | 74 PENDING\n",
            "Current time: 2025-11-01 15:44:07. Total running time: 3min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00006   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00007   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00008   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00009   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00010   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "69 more PENDING, 1 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00006 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00006 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=6055)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=6055)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=6055)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=6055)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=6055)\u001b[0m [1,  2000] loss: 2.274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:44:37,349\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00006\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=6055, ip=172.28.0.12, actor_id=b1da4df60f98f1678112229001000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00006 errored after 0 iterations at 2025-11-01 15:44:37. Total running time: 4min 0s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00006_6_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial status: 7 ERROR | 73 PENDING\n",
            "Current time: 2025-11-01 15:44:37. Total running time: 4min 1s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00007   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00008   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00009   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00010   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00011   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "68 more PENDING, 2 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00007 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00007 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=6344)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=6344)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=6344)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=6344)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=6344)\u001b[0m [1,  2000] loss: 2.269\n",
            "\n",
            "Trial status: 7 ERROR | 1 RUNNING | 72 PENDING\n",
            "Current time: 2025-11-01 15:45:07. Total running time: 4min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00007   RUNNING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00008   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00009   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00010   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00011   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00012   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "67 more PENDING, 2 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:45:10,919\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00007\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=6344, ip=172.28.0.12, actor_id=7e57b5df3bb2f8425cb6a79c01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00007 errored after 0 iterations at 2025-11-01 15:45:10. Total running time: 4min 34s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00007_7_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00008 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00008 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=6632)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=6632)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=6632)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=6632)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 8 ERROR | 1 RUNNING | 71 PENDING\n",
            "Current time: 2025-11-01 15:45:38. Total running time: 5min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00008   RUNNING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00009   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00010   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00011   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00012   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00013   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "66 more PENDING, 3 more ERROR\n",
            "\u001b[36m(func pid=6632)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:45:46,342\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00008\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=6632, ip=172.28.0.12, actor_id=2b49efae5b7ec88e3cfa797c01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00008 errored after 0 iterations at 2025-11-01 15:45:46. Total running time: 5min 9s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00008_8_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00009 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00009 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=6926)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=6926)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=6926)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=6926)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 9 ERROR | 1 RUNNING | 70 PENDING\n",
            "Current time: 2025-11-01 15:46:08. Total running time: 5min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00009   RUNNING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00010   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00011   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00012   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00013   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00014   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "65 more PENDING, 4 more ERROR\n",
            "\u001b[36m(func pid=6926)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:46:18,789\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00009\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=6926, ip=172.28.0.12, actor_id=f8596b7f23d50c4f660f52ea01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00009 errored after 0 iterations at 2025-11-01 15:46:18. Total running time: 5min 42s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00009_9_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00010 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00010 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=7210)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=7210)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=7210)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=7210)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 10 ERROR | 1 RUNNING | 69 PENDING\n",
            "Current time: 2025-11-01 15:46:38. Total running time: 6min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00010   RUNNING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00011   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00012   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00013   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00014   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00015   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "64 more PENDING, 5 more ERROR\n",
            "\u001b[36m(func pid=7210)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:46:55,119\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00010\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=7210, ip=172.28.0.12, actor_id=3c67ceee1388c7a4a3c8be1b01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00010 errored after 0 iterations at 2025-11-01 15:46:55. Total running time: 6min 18s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00010_10_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial status: 11 ERROR | 69 PENDING\n",
            "Current time: 2025-11-01 15:47:08. Total running time: 6min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00011   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00012   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00013   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00014   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00015   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "64 more PENDING, 6 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00011 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00011 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=7522)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=7522)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=7522)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=7522)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=7522)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:47:32,908\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00011\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=7522, ip=172.28.0.12, actor_id=8133cee14fb6b84fbff514eb01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00011 errored after 0 iterations at 2025-11-01 15:47:32. Total running time: 6min 56s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00011_11_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial status: 12 ERROR | 68 PENDING\n",
            "Current time: 2025-11-01 15:47:38. Total running time: 7min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00012   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00013   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00014   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00015   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00016   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "63 more PENDING, 7 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00012 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00012 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=7805)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=7805)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=7805)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=7805)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=7805)\u001b[0m [1,  2000] loss: 2.276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:48:07,782\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00012\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=7805, ip=172.28.0.12, actor_id=35a7096ca54c7cc46bf1d30501000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00012 errored after 0 iterations at 2025-11-01 15:48:07. Total running time: 7min 31s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00012_12_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial status: 13 ERROR | 67 PENDING\n",
            "Current time: 2025-11-01 15:48:08. Total running time: 7min 31s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00013   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00014   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00015   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00016   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00017   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "62 more PENDING, 8 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00013 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00013 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=8088)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=8088)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=8088)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=8088)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=8088)\u001b[0m [1,  2000] loss: 2.269\n",
            "\n",
            "Trial status: 13 ERROR | 1 RUNNING | 66 PENDING\n",
            "Current time: 2025-11-01 15:48:38. Total running time: 8min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00013   RUNNING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00014   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00015   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00016   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00017   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00018   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "61 more PENDING, 8 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:48:42,747\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00013\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=8088, ip=172.28.0.12, actor_id=074bc2cc980bc5122dba439d01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00013 errored after 0 iterations at 2025-11-01 15:48:42. Total running time: 8min 6s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00013_13_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00014 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00014 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=8378)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=8378)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=8378)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=8378)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 14 ERROR | 1 RUNNING | 65 PENDING\n",
            "Current time: 2025-11-01 15:49:08. Total running time: 8min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00014   RUNNING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00015   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00016   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00017   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00018   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00019   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "60 more PENDING, 9 more ERROR\n",
            "\u001b[36m(func pid=8378)\u001b[0m [1,  2000] loss: 2.292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:49:16,191\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00014\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=8378, ip=172.28.0.12, actor_id=0341b469f526785638b7568f01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00014 errored after 0 iterations at 2025-11-01 15:49:16. Total running time: 8min 39s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00014_14_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00015 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00015 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=8660)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=8660)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=8660)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=8660)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 15 ERROR | 1 RUNNING | 64 PENDING\n",
            "Current time: 2025-11-01 15:49:38. Total running time: 9min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00015   RUNNING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00016   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00017   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00018   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00019   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00020   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "59 more PENDING, 10 more ERROR\n",
            "\u001b[36m(func pid=8660)\u001b[0m [1,  2000] loss: 2.251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:49:51,516\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00015\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=8660, ip=172.28.0.12, actor_id=ae841d32c92ab6be3b7c276a01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00015 errored after 0 iterations at 2025-11-01 15:49:51. Total running time: 9min 15s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00015_15_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00016 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00016 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=8942)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=8942)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=8942)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=8942)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 16 ERROR | 1 RUNNING | 63 PENDING\n",
            "Current time: 2025-11-01 15:50:08. Total running time: 9min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00016   RUNNING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00017   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00018   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00019   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00020   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00021   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "58 more PENDING, 11 more ERROR\n",
            "\u001b[36m(func pid=8942)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:50:24,932\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00016\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=8942, ip=172.28.0.12, actor_id=2d156863047cf642e5a56b8301000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00016 errored after 0 iterations at 2025-11-01 15:50:24. Total running time: 9min 48s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00016_16_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00017 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00017 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=9232)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=9232)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=9232)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=9232)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 17 ERROR | 1 RUNNING | 62 PENDING\n",
            "Current time: 2025-11-01 15:50:38. Total running time: 10min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00017   RUNNING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00018   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00019   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00020   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00021   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00022   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "57 more PENDING, 12 more ERROR\n",
            "\u001b[36m(func pid=9232)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:51:00,438\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00017\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=9232, ip=172.28.0.12, actor_id=55ef02f909074459a3de389f01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00017 errored after 0 iterations at 2025-11-01 15:51:00. Total running time: 10min 23s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00017_17_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial status: 18 ERROR | 62 PENDING\n",
            "Current time: 2025-11-01 15:51:08. Total running time: 10min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00018   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00019   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00020   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00021   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00022   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "57 more PENDING, 13 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00018 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00018 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=9516)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=9516)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=9516)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=9516)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=9516)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:51:33,934\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00018\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=9516, ip=172.28.0.12, actor_id=0bbcb7daff4f3a521bd927c701000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00018 errored after 0 iterations at 2025-11-01 15:51:33. Total running time: 10min 57s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00018_18_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial status: 19 ERROR | 61 PENDING\n",
            "Current time: 2025-11-01 15:51:38. Total running time: 11min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00019   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00020   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00021   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00022   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00023   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "56 more PENDING, 14 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00019 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00019 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=9801)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=9801)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=9801)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=9801)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=9801)\u001b[0m [1,  2000] loss: 2.303\n",
            "\n",
            "Trial status: 19 ERROR | 1 RUNNING | 60 PENDING\n",
            "Current time: 2025-11-01 15:52:08. Total running time: 11min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00019   RUNNING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00020   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00021   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00022   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00023   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00024   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "55 more PENDING, 14 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:52:09,197\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00019\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=9801, ip=172.28.0.12, actor_id=2497884a0d8b5ca39e14e9b101000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00019 errored after 0 iterations at 2025-11-01 15:52:09. Total running time: 11min 32s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00019_19_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00020 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00020 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=10084)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=10084)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=10084)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=10084)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=10084)\u001b[0m [1,  2000] loss: 2.263\n",
            "\n",
            "Trial status: 20 ERROR | 1 RUNNING | 59 PENDING\n",
            "Current time: 2025-11-01 15:52:38. Total running time: 12min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00020   RUNNING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00021   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00022   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00023   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00024   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00025   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "54 more PENDING, 15 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:52:44,852\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00020\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=10084, ip=172.28.0.12, actor_id=c0be5d40fd168c9caf0da1e201000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00020 errored after 0 iterations at 2025-11-01 15:52:44. Total running time: 12min 8s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00020_20_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00021 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00021 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=10371)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=10371)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=10371)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=10371)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 21 ERROR | 1 RUNNING | 58 PENDING\n",
            "Current time: 2025-11-01 15:53:08. Total running time: 12min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00021   RUNNING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00022   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00023   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00024   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00025   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00026   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "53 more PENDING, 16 more ERROR\n",
            "\u001b[36m(func pid=10371)\u001b[0m [1,  2000] loss: 2.251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:53:18,261\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00021\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=10371, ip=172.28.0.12, actor_id=2e9426ba8a6e81247f6c806e01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00021 errored after 0 iterations at 2025-11-01 15:53:18. Total running time: 12min 41s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00021_21_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00022 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00022 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=10654)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=10654)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=10654)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=10654)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 22 ERROR | 1 RUNNING | 57 PENDING\n",
            "Current time: 2025-11-01 15:53:38. Total running time: 13min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00022   RUNNING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00023   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00024   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00025   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00026   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00027   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "52 more PENDING, 17 more ERROR\n",
            "\u001b[36m(func pid=10654)\u001b[0m [1,  2000] loss: 2.260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:53:53,881\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00022\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=10654, ip=172.28.0.12, actor_id=b0522282ca8b0dc6650922d401000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00022 errored after 0 iterations at 2025-11-01 15:53:53. Total running time: 13min 17s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00022_22_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00023 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00023 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=10937)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=10937)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=10937)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=10937)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 23 ERROR | 1 RUNNING | 56 PENDING\n",
            "Current time: 2025-11-01 15:54:08. Total running time: 13min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00023   RUNNING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00024   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00025   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00026   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00027   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00028   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "51 more PENDING, 18 more ERROR\n",
            "\u001b[36m(func pid=10937)\u001b[0m [1,  2000] loss: 2.270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:54:26,990\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00023\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=10937, ip=172.28.0.12, actor_id=29e5dde3e0ca56a1625d291b01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00023 errored after 0 iterations at 2025-11-01 15:54:26. Total running time: 13min 50s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00023_23_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00024 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00024 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "Trial status: 24 ERROR | 1 RUNNING | 55 PENDING\n",
            "Current time: 2025-11-01 15:54:38. Total running time: 14min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00024   RUNNING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00025   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00026   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00027   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00028   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00029   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "50 more PENDING, 19 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=11215)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=11215)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=11215)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=11215)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=11215)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:55:01,820\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00024\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=11215, ip=172.28.0.12, actor_id=1c44784fd63567153a4aa57401000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00024 errored after 0 iterations at 2025-11-01 15:55:01. Total running time: 14min 25s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00024_24_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial status: 25 ERROR | 55 PENDING\n",
            "Current time: 2025-11-01 15:55:08. Total running time: 14min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00025   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00026   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00027   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00028   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00029   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "50 more PENDING, 20 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00025 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00025 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=11503)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=11503)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=11503)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=11503)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=11503)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:55:35,214\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00025\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=11503, ip=172.28.0.12, actor_id=cd08682dd7d1944e9b339f4601000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00025 errored after 0 iterations at 2025-11-01 15:55:35. Total running time: 14min 58s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00025_25_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial status: 26 ERROR | 54 PENDING\n",
            "Current time: 2025-11-01 15:55:39. Total running time: 15min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00026   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00027   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00028   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00029   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00030   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "49 more PENDING, 21 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00026 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00026 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=11784)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=11784)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=11784)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=11784)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=11784)\u001b[0m [1,  2000] loss: 2.303\n",
            "\n",
            "Trial status: 26 ERROR | 1 RUNNING | 53 PENDING\n",
            "Current time: 2025-11-01 15:56:09. Total running time: 15min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00026   RUNNING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00027   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00028   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00029   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00030   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00031   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "48 more PENDING, 21 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:56:11,513\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00026\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=11784, ip=172.28.0.12, actor_id=f48754c7e8663a35bba8985501000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00026 errored after 0 iterations at 2025-11-01 15:56:11. Total running time: 15min 35s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00026_26_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00027 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00027 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=12075)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=12075)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=12075)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=12075)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=12075)\u001b[0m [1,  2000] loss: 2.303\n",
            "\n",
            "Trial status: 27 ERROR | 1 RUNNING | 52 PENDING\n",
            "Current time: 2025-11-01 15:56:39. Total running time: 16min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00027   RUNNING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00028   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00029   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00030   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00031   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00032   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "47 more PENDING, 22 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:56:45,824\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00027\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=12075, ip=172.28.0.12, actor_id=edc23dce690b54060ecbac7201000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00027 errored after 0 iterations at 2025-11-01 15:56:45. Total running time: 16min 9s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00027_27_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00028 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00028 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=12357)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=12357)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=12357)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=12357)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 28 ERROR | 1 RUNNING | 51 PENDING\n",
            "Current time: 2025-11-01 15:57:09. Total running time: 16min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00028   RUNNING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00029   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00030   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00031   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00032   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00033   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "46 more PENDING, 23 more ERROR\n",
            "\u001b[36m(func pid=12357)\u001b[0m [1,  2000] loss: 2.257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:57:20,924\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00028\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=12357, ip=172.28.0.12, actor_id=cd3cc33359454c3ebacccade01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00028 errored after 0 iterations at 2025-11-01 15:57:20. Total running time: 16min 44s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00028_28_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00029 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00029 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=12639)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=12639)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=12639)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=12639)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 29 ERROR | 1 RUNNING | 50 PENDING\n",
            "Current time: 2025-11-01 15:57:39. Total running time: 17min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00029   RUNNING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00030   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00031   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00032   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00033   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00034   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "45 more PENDING, 24 more ERROR\n",
            "\u001b[36m(func pid=12639)\u001b[0m [1,  2000] loss: 2.284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:57:56,491\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00029\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=12639, ip=172.28.0.12, actor_id=199464eab563a4e5d3d187d101000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00029 errored after 0 iterations at 2025-11-01 15:57:56. Total running time: 17min 20s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00029_29_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00030 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00030 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=12929)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=12929)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=12929)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=12929)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 30 ERROR | 1 RUNNING | 49 PENDING\n",
            "Current time: 2025-11-01 15:58:09. Total running time: 17min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00030   RUNNING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00031   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00032   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00033   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00034   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00035   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "44 more PENDING, 25 more ERROR\n",
            "\u001b[36m(func pid=12929)\u001b[0m [1,  2000] loss: 2.298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:58:30,377\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00030\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=12929, ip=172.28.0.12, actor_id=00e3ab2fe345622b3460d27001000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00030 errored after 0 iterations at 2025-11-01 15:58:30. Total running time: 17min 53s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00030_30_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 31 ERROR | 49 PENDING\n",
            "Current time: 2025-11-01 15:58:39. Total running time: 18min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00031   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00032   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00033   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00034   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00035   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "44 more PENDING, 26 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00031 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00031 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=13216)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=13216)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=13216)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=13216)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=13216)\u001b[0m [1,  2000] loss: 2.286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:59:06,914\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00031\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=13216, ip=172.28.0.12, actor_id=9284c267db9ec86530e3124701000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00031 errored after 0 iterations at 2025-11-01 15:59:06. Total running time: 18min 30s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00031_31_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 32 ERROR | 48 PENDING\n",
            "Current time: 2025-11-01 15:59:09. Total running time: 18min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00032   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00033   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00034   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00035   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00036   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "43 more PENDING, 27 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00032 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00032 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=13510)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=13510)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=13510)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=13510)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=13510)\u001b[0m [1,  2000] loss: 2.303\n",
            "\n",
            "Trial status: 32 ERROR | 1 RUNNING | 47 PENDING\n",
            "Current time: 2025-11-01 15:59:39. Total running time: 19min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00032   RUNNING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00033   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00034   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00035   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00036   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00037   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "42 more PENDING, 27 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 15:59:41,961\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00032\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=13510, ip=172.28.0.12, actor_id=6340ce4c3e278061fbd9605f01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00032 errored after 0 iterations at 2025-11-01 15:59:41. Total running time: 19min 5s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00032_32_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00033 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00033 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=13803)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=13803)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=13803)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=13803)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=13803)\u001b[0m [1,  2000] loss: 2.303\n",
            "\n",
            "Trial status: 33 ERROR | 1 RUNNING | 46 PENDING\n",
            "Current time: 2025-11-01 16:00:09. Total running time: 19min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00033   RUNNING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00034   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00035   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00036   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00037   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00038   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "41 more PENDING, 28 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:00:16,913\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00033\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=13803, ip=172.28.0.12, actor_id=2e25353d1be3e725825526cf01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00033 errored after 0 iterations at 2025-11-01 16:00:16. Total running time: 19min 40s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00033_33_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00034 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00034 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=14097)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=14097)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=14097)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=14097)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 34 ERROR | 1 RUNNING | 45 PENDING\n",
            "Current time: 2025-11-01 16:00:39. Total running time: 20min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00034   RUNNING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00035   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00036   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00037   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00038   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00039   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "40 more PENDING, 29 more ERROR\n",
            "\u001b[36m(func pid=14097)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:00:53,626\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00034\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=14097, ip=172.28.0.12, actor_id=c980f6174433aedd7d3dc35f01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00034 errored after 0 iterations at 2025-11-01 16:00:53. Total running time: 20min 17s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00034_34_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00035 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00035 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=14394)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=14394)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=14394)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=14394)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 35 ERROR | 1 RUNNING | 44 PENDING\n",
            "Current time: 2025-11-01 16:01:09. Total running time: 20min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00035   RUNNING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00036   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00037   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00038   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00039   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00040   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "39 more PENDING, 30 more ERROR\n",
            "\u001b[36m(func pid=14394)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:01:29,015\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00035\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=14394, ip=172.28.0.12, actor_id=fc4f2655f89d0e802b7a041401000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00035 errored after 0 iterations at 2025-11-01 16:01:29. Total running time: 20min 52s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00035_35_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00036 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00036 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n",
            "\n",
            "Trial status: 36 ERROR | 1 RUNNING | 43 PENDING\n",
            "Current time: 2025-11-01 16:01:39. Total running time: 21min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00036   RUNNING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00037   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00038   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00039   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00040   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00041   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "38 more PENDING, 31 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=14691)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=14691)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=14691)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=14691)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=14691)\u001b[0m [1,  2000] loss: 2.265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:02:05,336\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00036\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=14691, ip=172.28.0.12, actor_id=5c0bef5c6aa2b018bbc852a901000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00036 errored after 0 iterations at 2025-11-01 16:02:05. Total running time: 21min 28s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00036_36_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 37 ERROR | 43 PENDING\n",
            "Current time: 2025-11-01 16:02:09. Total running time: 21min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00037   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00038   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00039   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00040   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00041   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "38 more PENDING, 32 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00037 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00037 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=14984)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=14984)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=14984)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=14984)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=14984)\u001b[0m [1,  2000] loss: 2.245\n",
            "\n",
            "Trial status: 37 ERROR | 1 RUNNING | 42 PENDING\n",
            "Current time: 2025-11-01 16:02:39. Total running time: 22min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00037   RUNNING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00038   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00039   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00040   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00041   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00042   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "37 more PENDING, 32 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=14984)\u001b[0m [2025-11-01 16:02:41,263 E 14984 15022] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "2025-11-01 16:02:41,672\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00037\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=14984, ip=172.28.0.12, actor_id=f691a952a94501f12986b23601000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00037 errored after 0 iterations at 2025-11-01 16:02:41. Total running time: 22min 5s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00037_37_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00038 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00038 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=15289)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=15289)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=15289)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=15289)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=15289)\u001b[0m [1,  2000] loss: 2.295\n",
            "\n",
            "Trial status: 38 ERROR | 1 RUNNING | 41 PENDING\n",
            "Current time: 2025-11-01 16:03:10. Total running time: 22min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00038   RUNNING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00039   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00040   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00041   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00042   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00043   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "36 more PENDING, 33 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:03:17,448\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00038\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=15289, ip=172.28.0.12, actor_id=f356d8700583501f3beeff4a01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00038 errored after 0 iterations at 2025-11-01 16:03:17. Total running time: 22min 40s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00038_38_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00039 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00039 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=15582)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=15582)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=15582)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=15582)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 39 ERROR | 1 RUNNING | 40 PENDING\n",
            "Current time: 2025-11-01 16:03:40. Total running time: 23min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00039   RUNNING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00040   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00041   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00042   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00043   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00044   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "35 more PENDING, 34 more ERROR\n",
            "\u001b[36m(func pid=15582)\u001b[0m [1,  2000] loss: 2.260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:03:54,093\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00039\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=15582, ip=172.28.0.12, actor_id=9174baf4d67f2b3b8852441401000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00039 errored after 0 iterations at 2025-11-01 16:03:54. Total running time: 23min 17s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00039_39_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00040 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00040 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=15881)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=15881)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=15881)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=15881)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 40 ERROR | 1 RUNNING | 39 PENDING\n",
            "Current time: 2025-11-01 16:04:10. Total running time: 23min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00040   RUNNING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00041   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00042   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00043   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00044   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00045   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "34 more PENDING, 35 more ERROR\n",
            "\u001b[36m(func pid=15881)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=15881)\u001b[0m [2025-11-01 16:04:30,502 E 15881 15922] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "2025-11-01 16:04:31,522\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00040\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=15881, ip=172.28.0.12, actor_id=9a2783aa30e8bc05af6bb6a001000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00040 errored after 0 iterations at 2025-11-01 16:04:31. Total running time: 23min 55s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00040_40_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 41 ERROR | 39 PENDING\n",
            "Current time: 2025-11-01 16:04:40. Total running time: 24min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00041   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00042   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00043   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00044   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00045   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "34 more PENDING, 36 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00041 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00041 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=16182)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=16182)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=16182)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=16182)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=16182)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:05:06,467\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00041\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=16182, ip=172.28.0.12, actor_id=012f9fe3dac4e1ee871b60f601000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00041 errored after 0 iterations at 2025-11-01 16:05:06. Total running time: 24min 29s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00041_41_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 42 ERROR | 38 PENDING\n",
            "Current time: 2025-11-01 16:05:10. Total running time: 24min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00042   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00043   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00044   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00045   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00046   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "33 more PENDING, 37 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00042 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00042 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=16478)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=16478)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=16478)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=16478)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=16478)\u001b[0m [1,  2000] loss: 2.303\n",
            "\n",
            "Trial status: 42 ERROR | 1 RUNNING | 37 PENDING\n",
            "Current time: 2025-11-01 16:05:40. Total running time: 25min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00042   RUNNING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00043   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00044   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00045   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00046   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00047   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "32 more PENDING, 37 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:05:43,501\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00042\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=16478, ip=172.28.0.12, actor_id=63875f63f3455c6ff84ebf2101000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n",
            "\u001b[36m(func pid=16478)\u001b[0m [2025-11-01 16:05:43,460 E 16478 16516] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00042 errored after 0 iterations at 2025-11-01 16:05:43. Total running time: 25min 7s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00042_42_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00043 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00043 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=16778)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=16778)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=16778)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=16778)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 43 ERROR | 1 RUNNING | 36 PENDING\n",
            "Current time: 2025-11-01 16:06:10. Total running time: 25min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00043   RUNNING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00044   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00045   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00046   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00047   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00048   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "31 more PENDING, 38 more ERROR\n",
            "\u001b[36m(func pid=16778)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=16778)\u001b[0m [2025-11-01 16:06:20,959 E 16778 16823] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "2025-11-01 16:06:21,756\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00043\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=16778, ip=172.28.0.12, actor_id=ca63e45f785c63f38c24123101000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00043 errored after 0 iterations at 2025-11-01 16:06:21. Total running time: 25min 45s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00043_43_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00044 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00044 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=17086)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=17086)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=17086)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=17086)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 44 ERROR | 1 RUNNING | 35 PENDING\n",
            "Current time: 2025-11-01 16:06:40. Total running time: 26min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00044   RUNNING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00045   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00046   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00047   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00048   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00049   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "30 more PENDING, 39 more ERROR\n",
            "\u001b[36m(func pid=17086)\u001b[0m [1,  2000] loss: 2.254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=17086)\u001b[0m [2025-11-01 16:06:58,433 E 17086 17124] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "2025-11-01 16:06:59,485\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00044\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=17086, ip=172.28.0.12, actor_id=c168d2ad4b05b4b4200bde6b01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00044 errored after 0 iterations at 2025-11-01 16:06:59. Total running time: 26min 23s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00044_44_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00045 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00045 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n",
            "\n",
            "Trial status: 45 ERROR | 1 RUNNING | 34 PENDING\n",
            "Current time: 2025-11-01 16:07:10. Total running time: 26min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00045   RUNNING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00046   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00047   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00048   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00049   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00050   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "29 more PENDING, 40 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=17388)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=17388)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=17388)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=17388)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=17388)\u001b[0m [1,  2000] loss: 2.270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:07:35,607\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00045\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=17388, ip=172.28.0.12, actor_id=e7205146d2b477a8777b643e01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00045 errored after 0 iterations at 2025-11-01 16:07:35. Total running time: 26min 59s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00045_45_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 46 ERROR | 34 PENDING\n",
            "Current time: 2025-11-01 16:07:40. Total running time: 27min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00046   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00047   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00048   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00049   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00050   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "29 more PENDING, 41 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00046 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00046 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=17688)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=17688)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=17688)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=17688)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=17688)\u001b[0m [1,  2000] loss: 2.286\n",
            "\n",
            "Trial status: 46 ERROR | 1 RUNNING | 33 PENDING\n",
            "Current time: 2025-11-01 16:08:10. Total running time: 27min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00046   RUNNING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00047   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00048   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00049   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00050   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00051   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "28 more PENDING, 41 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:08:12,182\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00046\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=17688, ip=172.28.0.12, actor_id=12e979097b3c010766af82e601000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00046 errored after 0 iterations at 2025-11-01 16:08:12. Total running time: 27min 35s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00046_46_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00047 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00047 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=17980)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=17980)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=17980)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=17980)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=17980)\u001b[0m [1,  2000] loss: 2.255\n",
            "\n",
            "Trial status: 47 ERROR | 1 RUNNING | 32 PENDING\n",
            "Current time: 2025-11-01 16:08:40. Total running time: 28min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00047   RUNNING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00048   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00049   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00050   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00051   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00052   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "27 more PENDING, 42 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:08:47,116\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00047\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=17980, ip=172.28.0.12, actor_id=95668031a6bf2e98cb9ac55301000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00047 errored after 0 iterations at 2025-11-01 16:08:47. Total running time: 28min 10s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00047_47_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00048 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00048 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=18278)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=18278)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=18278)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=18278)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 48 ERROR | 1 RUNNING | 31 PENDING\n",
            "Current time: 2025-11-01 16:09:10. Total running time: 28min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00048   RUNNING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00049   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00050   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00051   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00052   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00053   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "26 more PENDING, 43 more ERROR\n",
            "\u001b[36m(func pid=18278)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:09:23,362\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00048\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=18278, ip=172.28.0.12, actor_id=c0bc1a54af6d3a7d964187d501000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00048 errored after 0 iterations at 2025-11-01 16:09:23. Total running time: 28min 46s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00048_48_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00049 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00049 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=18576)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=18576)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=18576)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=18576)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 49 ERROR | 1 RUNNING | 30 PENDING\n",
            "Current time: 2025-11-01 16:09:40. Total running time: 29min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00049   RUNNING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00050   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00051   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00052   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00053   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00054   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "25 more PENDING, 44 more ERROR\n",
            "\u001b[36m(func pid=18576)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:10:00,645\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00049\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=18576, ip=172.28.0.12, actor_id=e56be7bc9231201f620e0faf01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00049 errored after 0 iterations at 2025-11-01 16:10:00. Total running time: 29min 24s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00049_49_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00050 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00050 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "Trial status: 50 ERROR | 1 RUNNING | 29 PENDING\n",
            "Current time: 2025-11-01 16:10:10. Total running time: 29min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00050   RUNNING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00051   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00052   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00053   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00054   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00055   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "24 more PENDING, 45 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=18872)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=18872)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=18872)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=18872)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=18872)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:10:35,167\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00050\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=18872, ip=172.28.0.12, actor_id=df577120daeb32847c41461701000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00050 errored after 0 iterations at 2025-11-01 16:10:35. Total running time: 29min 58s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00050_50_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 51 ERROR | 29 PENDING\n",
            "Current time: 2025-11-01 16:10:41. Total running time: 30min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00051   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00052   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00053   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00054   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00055   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "24 more PENDING, 46 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00051 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00051 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=19170)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=19170)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=19170)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=19170)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=19170)\u001b[0m [1,  2000] loss: 2.303\n",
            "\n",
            "Trial status: 51 ERROR | 1 RUNNING | 28 PENDING\n",
            "Current time: 2025-11-01 16:11:11. Total running time: 30min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00051   RUNNING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00052   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00053   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00054   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00055   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00056   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "23 more PENDING, 46 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:11:11,582\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00051\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19170, ip=172.28.0.12, actor_id=e9c859b3c9a7b4751afa23fc01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00051 errored after 0 iterations at 2025-11-01 16:11:11. Total running time: 30min 35s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00051_51_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00052 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00052 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=19458)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=19458)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=19458)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=19458)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=19458)\u001b[0m [1,  2000] loss: 2.272\n",
            "\n",
            "Trial status: 52 ERROR | 1 RUNNING | 27 PENDING\n",
            "Current time: 2025-11-01 16:11:41. Total running time: 31min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00052   RUNNING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00053   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00054   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00055   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00056   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00057   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "22 more PENDING, 47 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:11:47,925\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00052\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19458, ip=172.28.0.12, actor_id=3647fb2557f381b4997931b401000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00052 errored after 0 iterations at 2025-11-01 16:11:47. Total running time: 31min 11s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00052_52_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00053 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00053 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=19760)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=19760)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=19760)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=19760)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 53 ERROR | 1 RUNNING | 26 PENDING\n",
            "Current time: 2025-11-01 16:12:11. Total running time: 31min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00053   RUNNING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00054   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00055   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00056   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00057   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00058   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "21 more PENDING, 48 more ERROR\n",
            "\u001b[36m(func pid=19760)\u001b[0m [1,  2000] loss: 2.288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:12:23,207\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00053\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19760, ip=172.28.0.12, actor_id=5e0ff1d0ed12262f98b0462501000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00053 errored after 0 iterations at 2025-11-01 16:12:23. Total running time: 31min 46s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00053_53_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00054 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00054 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=20060)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=20060)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=20060)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=20060)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 54 ERROR | 1 RUNNING | 25 PENDING\n",
            "Current time: 2025-11-01 16:12:41. Total running time: 32min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00054   RUNNING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00055   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00056   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00057   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00058   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00059   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "20 more PENDING, 49 more ERROR\n",
            "\u001b[36m(func pid=20060)\u001b[0m [1,  2000] loss: 2.265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:13:00,193\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00054\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=20060, ip=172.28.0.12, actor_id=1f3ee8858d29b46cc6d972ed01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00054 errored after 0 iterations at 2025-11-01 16:13:00. Total running time: 32min 23s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00054_54_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00055 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00055 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n",
            "\n",
            "Trial status: 55 ERROR | 1 RUNNING | 24 PENDING\n",
            "Current time: 2025-11-01 16:13:11. Total running time: 32min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00055   RUNNING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00056   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00057   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00058   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00059   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00060   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "19 more PENDING, 50 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=20356)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=20356)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=20356)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=20356)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=20356)\u001b[0m [1,  2000] loss: 2.259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=20356)\u001b[0m [2025-11-01 16:13:36,499 E 20356 20394] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "2025-11-01 16:13:37,284\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00055\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=20356, ip=172.28.0.12, actor_id=b8a77b4f8096eebb6db0382501000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00055 errored after 0 iterations at 2025-11-01 16:13:37. Total running time: 33min 0s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00055_55_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 56 ERROR | 24 PENDING\n",
            "Current time: 2025-11-01 16:13:41. Total running time: 33min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00056   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00057   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00058   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00059   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00060   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "19 more PENDING, 51 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00056 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00056 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=20653)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=20653)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=20653)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=20653)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=20653)\u001b[0m [1,  2000] loss: 2.303\n",
            "\n",
            "Trial status: 56 ERROR | 1 RUNNING | 23 PENDING\n",
            "Current time: 2025-11-01 16:14:11. Total running time: 33min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00056   RUNNING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00057   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00058   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00059   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00060   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00061   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "18 more PENDING, 51 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:14:12,074\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00056\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=20653, ip=172.28.0.12, actor_id=c2f9bcc1f216cbf2fab164db01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00056 errored after 0 iterations at 2025-11-01 16:14:12. Total running time: 33min 35s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00056_56_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00057 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00057 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=20949)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=20949)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=20949)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=20949)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=20949)\u001b[0m [1,  2000] loss: 2.303\n",
            "\n",
            "Trial status: 57 ERROR | 1 RUNNING | 22 PENDING\n",
            "Current time: 2025-11-01 16:14:41. Total running time: 34min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00057   RUNNING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00058   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00059   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00060   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00061   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00062   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "17 more PENDING, 52 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:14:48,627\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00057\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=20949, ip=172.28.0.12, actor_id=370f41b29580b89539940f7201000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00057 errored after 0 iterations at 2025-11-01 16:14:48. Total running time: 34min 12s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00057_57_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00058 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00058 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=21248)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=21248)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=21248)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=21248)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 58 ERROR | 1 RUNNING | 21 PENDING\n",
            "Current time: 2025-11-01 16:15:11. Total running time: 34min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00058   RUNNING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00059   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00060   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00061   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00062   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00063   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "16 more PENDING, 53 more ERROR\n",
            "\u001b[36m(func pid=21248)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:15:24,998\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00058\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=21248, ip=172.28.0.12, actor_id=1861f3eae4bdc6554908adbf01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n",
            "\u001b[36m(func pid=21248)\u001b[0m [2025-11-01 16:15:24,962 E 21248 21287] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00058 errored after 0 iterations at 2025-11-01 16:15:25. Total running time: 34min 48s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00058_58_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00059 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00059 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=21549)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=21549)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=21549)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=21549)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 59 ERROR | 1 RUNNING | 20 PENDING\n",
            "Current time: 2025-11-01 16:15:41. Total running time: 35min 5s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00059   RUNNING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00060   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00061   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00062   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00063   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00064   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "15 more PENDING, 54 more ERROR\n",
            "\u001b[36m(func pid=21549)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:16:00,246\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00059\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=21549, ip=172.28.0.12, actor_id=3db5503cdaff2a7e822ba48a01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00059 errored after 0 iterations at 2025-11-01 16:16:00. Total running time: 35min 23s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00059_59_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 60 ERROR | 20 PENDING\n",
            "Current time: 2025-11-01 16:16:11. Total running time: 35min 35s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00060   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00061   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00062   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00063   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00064   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "15 more PENDING, 55 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00060 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00060 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=21840)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=21840)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=21840)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=21840)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=21840)\u001b[0m [1,  2000] loss: 2.278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:16:37,940\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00060\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=21840, ip=172.28.0.12, actor_id=e27aaa6a2a046ec208ee864301000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00060 errored after 0 iterations at 2025-11-01 16:16:37. Total running time: 36min 1s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00060_60_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 61 ERROR | 19 PENDING\n",
            "Current time: 2025-11-01 16:16:41. Total running time: 36min 5s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00061   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00062   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00063   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00064   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00065   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "14 more PENDING, 56 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00061 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00061 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=22141)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=22141)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=22141)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=22141)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=22141)\u001b[0m [1,  2000] loss: 2.271\n",
            "\n",
            "Trial status: 61 ERROR | 1 RUNNING | 18 PENDING\n",
            "Current time: 2025-11-01 16:17:11. Total running time: 36min 35s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00061   RUNNING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00062   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00063   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00064   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00065   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00066   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "13 more PENDING, 56 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=22141)\u001b[0m [2025-11-01 16:17:14,254 E 22141 22183] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "2025-11-01 16:17:14,689\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00061\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=22141, ip=172.28.0.12, actor_id=3dbc1d606b6e94168cb3a66f01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00061 errored after 0 iterations at 2025-11-01 16:17:14. Total running time: 36min 38s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00061_61_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00062 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00062 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=22442)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=22442)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=22442)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=22442)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 62 ERROR | 1 RUNNING | 17 PENDING\n",
            "Current time: 2025-11-01 16:17:41. Total running time: 37min 5s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00062   RUNNING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00063   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00064   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00065   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00066   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00067   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "12 more PENDING, 57 more ERROR\n",
            "\u001b[36m(func pid=22442)\u001b[0m [1,  2000] loss: 2.265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:17:50,012\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00062\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=22442, ip=172.28.0.12, actor_id=49b87c8df5852602ff0433c701000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00062 errored after 0 iterations at 2025-11-01 16:17:50. Total running time: 37min 13s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00062_62_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00063 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00063 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=22735)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=22735)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=22735)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=22735)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 63 ERROR | 1 RUNNING | 16 PENDING\n",
            "Current time: 2025-11-01 16:18:11. Total running time: 37min 35s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00063   RUNNING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00064   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00065   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00066   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00067   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00068   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "11 more PENDING, 58 more ERROR\n",
            "\u001b[36m(func pid=22735)\u001b[0m [1,  2000] loss: 2.243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:18:27,204\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00063\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=22735, ip=172.28.0.12, actor_id=baa3aa1c763a551b5aec918001000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00063 errored after 0 iterations at 2025-11-01 16:18:27. Total running time: 37min 50s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00063_63_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00064 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00064 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=23039)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=23039)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=23039)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=23039)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 64 ERROR | 1 RUNNING | 15 PENDING\n",
            "Current time: 2025-11-01 16:18:41. Total running time: 38min 5s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00064   RUNNING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00065   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00066   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00067   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00068   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00069   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "10 more PENDING, 59 more ERROR\n",
            "\u001b[36m(func pid=23039)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=23039)\u001b[0m [2025-11-01 16:19:04,086 E 23039 23079] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "2025-11-01 16:19:04,454\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00064\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=23039, ip=172.28.0.12, actor_id=6afaac01f6c3ad7048ac891001000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00064 errored after 0 iterations at 2025-11-01 16:19:04. Total running time: 38min 27s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00064_64_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 65 ERROR | 15 PENDING\n",
            "Current time: 2025-11-01 16:19:11. Total running time: 38min 35s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00065   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00066   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00067   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00068   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00069   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "10 more PENDING, 60 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00065 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00065 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=23336)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=23336)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=23336)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=23336)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=23336)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=23336)\u001b[0m [2025-11-01 16:19:40,240 E 23336 23377] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 65 ERROR | 1 RUNNING | 14 PENDING\n",
            "Current time: 2025-11-01 16:19:41. Total running time: 39min 5s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00065   RUNNING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00066   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00067   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00068   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00069   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00070   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "9 more PENDING, 60 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:19:48,511\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00065\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=23336, ip=172.28.0.12, actor_id=745f4ba8479f12197fd5b75201000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00065 errored after 0 iterations at 2025-11-01 16:19:48. Total running time: 39min 12s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00065_65_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00066 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00066 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=23667)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=23667)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=23667)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=23667)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 66 ERROR | 1 RUNNING | 13 PENDING\n",
            "Current time: 2025-11-01 16:20:11. Total running time: 39min 35s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00066   RUNNING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00067   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00068   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00069   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00070   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00071   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "8 more PENDING, 61 more ERROR\n",
            "\u001b[36m(func pid=23667)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=23667)\u001b[0m [2025-11-01 16:20:24,692 E 23667 23710] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "2025-11-01 16:20:29,577\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00066\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=23667, ip=172.28.0.12, actor_id=c704dc5a4907c573a7f7a1f901000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00066 errored after 0 iterations at 2025-11-01 16:20:29. Total running time: 39min 53s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00066_66_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00067 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00067 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=23988)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=23988)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 67 ERROR | 1 RUNNING | 12 PENDING\n",
            "Current time: 2025-11-01 16:20:42. Total running time: 40min 5s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00067   RUNNING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00068   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00069   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00070   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00071   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00072   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "7 more PENDING, 62 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=23988)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=23988)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=23988)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:21:04,372\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00067\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=23988, ip=172.28.0.12, actor_id=82942abe8bf7401ec6b9fc3e01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00067 errored after 0 iterations at 2025-11-01 16:21:04. Total running time: 40min 27s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00067_67_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 68 ERROR | 12 PENDING\n",
            "Current time: 2025-11-01 16:21:12. Total running time: 40min 35s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00068   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00069   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00070   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00071   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00072   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "7 more PENDING, 63 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00068 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00068 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=24287)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=24287)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=24287)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=24287)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=24287)\u001b[0m [1,  2000] loss: 2.277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=24287)\u001b[0m [2025-11-01 16:21:41,731 E 24287 24324] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 68 ERROR | 1 RUNNING | 11 PENDING\n",
            "Current time: 2025-11-01 16:21:42. Total running time: 41min 5s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00068   RUNNING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00069   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00070   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00071   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00072   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00073   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "6 more PENDING, 63 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:21:43,282\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00068\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=24287, ip=172.28.0.12, actor_id=ae44df0b3345f665684ac57301000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00068 errored after 0 iterations at 2025-11-01 16:21:43. Total running time: 41min 6s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00068_68_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00069 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00069 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=24598)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=24598)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=24598)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=24598)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 69 ERROR | 1 RUNNING | 10 PENDING\n",
            "Current time: 2025-11-01 16:22:12. Total running time: 41min 35s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00069   RUNNING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00070   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00071   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00072   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00073   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00074   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "5 more PENDING, 64 more ERROR\n",
            "\u001b[36m(func pid=24598)\u001b[0m [1,  2000] loss: 2.302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:22:21,732\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00069\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=24598, ip=172.28.0.12, actor_id=0256856577c7f5a7c3024ad701000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00069 errored after 0 iterations at 2025-11-01 16:22:21. Total running time: 41min 45s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00069_69_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00070 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00070 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=24889)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=24889)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=24889)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=24889)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 70 ERROR | 1 RUNNING | 9 PENDING\n",
            "Current time: 2025-11-01 16:22:42. Total running time: 42min 5s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00070   RUNNING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00071   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00072   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00073   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00074   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00075   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "4 more PENDING, 65 more ERROR\n",
            "\u001b[36m(func pid=24889)\u001b[0m [1,  2000] loss: 2.265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:22:58,123\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00070\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=24889, ip=172.28.0.12, actor_id=b17f677013ef8c288ab9f97101000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00070 errored after 0 iterations at 2025-11-01 16:22:58. Total running time: 42min 21s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00070_70_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00071 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00071 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=25184)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=25184)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=25184)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=25184)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 71 ERROR | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-11-01 16:23:12. Total running time: 42min 35s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00071   RUNNING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00072   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00073   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00074   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00075   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00076   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "3 more PENDING, 66 more ERROR\n",
            "\u001b[36m(func pid=25184)\u001b[0m [1,  2000] loss: 2.269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:23:33,166\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00071\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=25184, ip=172.28.0.12, actor_id=ffd249f6272365dcde62d0a301000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00071 errored after 0 iterations at 2025-11-01 16:23:33. Total running time: 42min 56s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00071_71_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 72 ERROR | 8 PENDING\n",
            "Current time: 2025-11-01 16:23:42. Total running time: 43min 5s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00072   PENDING      32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00073   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00074   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00075   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00076   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "3 more PENDING, 67 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00072 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00072 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=25479)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=25479)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=25479)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=25479)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=25479)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:24:09,370\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00072\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=25479, ip=172.28.0.12, actor_id=cb84cb98f3568aeacc7c849101000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00072 errored after 0 iterations at 2025-11-01 16:24:09. Total running time: 43min 32s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00072_72_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 73 ERROR | 7 PENDING\n",
            "Current time: 2025-11-01 16:24:12. Total running time: 43min 35s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00073   PENDING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00074   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00075   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00076   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00077   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "2 more PENDING, 68 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00073 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00073 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           16 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=25772)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=25772)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=25772)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=25772)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=25772)\u001b[0m [1,  2000] loss: 2.303\n",
            "\n",
            "Trial status: 73 ERROR | 1 RUNNING | 6 PENDING\n",
            "Current time: 2025-11-01 16:24:42. Total running time: 44min 5s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00073   RUNNING      64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00074   PENDING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00075   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00076   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00077   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00078   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "1 more PENDING, 68 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=25772)\u001b[0m [2025-11-01 16:24:46,031 E 25772 25814] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "2025-11-01 16:24:46,345\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00073\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=25772, ip=172.28.0.12, actor_id=706c3f51d368ca7195effed001000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00073 errored after 0 iterations at 2025-11-01 16:24:46. Total running time: 44min 9s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00073_73_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00074 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00074 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           32 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=26079)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=26079)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=26079)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=26079)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 74 ERROR | 1 RUNNING | 5 PENDING\n",
            "Current time: 2025-11-01 16:25:12. Total running time: 44min 36s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00074   RUNNING      32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00075   PENDING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00076   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00077   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00078   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00079   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "69 more ERROR\n",
            "\u001b[36m(func pid=26079)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:25:21,964\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00074\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=26079, ip=172.28.0.12, actor_id=217edc70bb04bf06a75bf81001000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00074 errored after 0 iterations at 2025-11-01 16:25:21. Total running time: 44min 45s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00074_74_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00075 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_1c521_00075 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                   16 |\n",
            "| l1                                           64 |\n",
            "| l2                                           32 |\n",
            "| lr                                       0.0001 |\n",
            "+-------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=26375)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=26375)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=26375)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=26375)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 75 ERROR | 1 RUNNING | 4 PENDING\n",
            "Current time: 2025-11-01 16:25:42. Total running time: 45min 6s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00075   RUNNING      64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00076   PENDING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00077   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00078   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00079   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "70 more ERROR\n",
            "\u001b[36m(func pid=26375)\u001b[0m [1,  2000] loss: 2.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:25:59,099\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00075\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=26375, ip=172.28.0.12, actor_id=fcc61853bcb0742a61982e0801000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00075 errored after 0 iterations at 2025-11-01 16:25:59. Total running time: 45min 22s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00075_75_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00076 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00076 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=26672)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=26672)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 76 ERROR | 1 RUNNING | 3 PENDING\n",
            "Current time: 2025-11-01 16:26:12. Total running time: 45min 36s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00076   RUNNING      32     16   0.01               16 |\n",
            "| train_cifar_1c521_00077   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00078   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00079   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "71 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=26672)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=26672)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=26672)\u001b[0m [1,  2000] loss: 2.257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:26:36,310\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00076\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=26672, ip=172.28.0.12, actor_id=d3a5d673fdf85a25a094be4e01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00076 errored after 0 iterations at 2025-11-01 16:26:36. Total running time: 45min 59s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00076_76_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 77 ERROR | 3 PENDING\n",
            "Current time: 2025-11-01 16:26:42. Total running time: 46min 6s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00077   PENDING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00078   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00079   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "72 more ERROR\n",
            "\n",
            "Trial train_cifar_1c521_00077 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00077 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         16 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=26980)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=26980)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=26980)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=26980)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=26980)\u001b[0m [1,  2000] loss: 2.251\n",
            "\n",
            "Trial status: 77 ERROR | 1 RUNNING | 2 PENDING\n",
            "Current time: 2025-11-01 16:27:12. Total running time: 46min 36s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00077   RUNNING      64     16   0.01               16 |\n",
            "| train_cifar_1c521_00078   PENDING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00079   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "72 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=26980)\u001b[0m [2025-11-01 16:27:12,836 E 26980 27017] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "2025-11-01 16:27:13,250\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00077\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=26980, ip=172.28.0.12, actor_id=063b2eafdccaac402916195901000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00077 errored after 0 iterations at 2025-11-01 16:27:13. Total running time: 46min 36s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00077_77_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00078 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00078 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         32 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=27280)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=27280)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=27280)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=27280)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=27280)\u001b[0m [1,  2000] loss: 2.276\n",
            "\n",
            "Trial status: 78 ERROR | 1 RUNNING | 1 PENDING\n",
            "Current time: 2025-11-01 16:27:42. Total running time: 47min 6s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00078   RUNNING      32     32   0.01               16 |\n",
            "| train_cifar_1c521_00079   PENDING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "73 more ERROR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:27:49,466\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00078\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=27280, ip=172.28.0.12, actor_id=f3aea9f30bde2178083e1f4f01000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00078 errored after 0 iterations at 2025-11-01 16:27:49. Total running time: 47min 12s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00078_78_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial train_cifar_1c521_00079 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_cifar_1c521_00079 config          |\n",
            "+-----------------------------------------------+\n",
            "| batch_size                                 16 |\n",
            "| l1                                         64 |\n",
            "| l2                                         32 |\n",
            "| lr                                       0.01 |\n",
            "+-----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=27577)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=27577)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(func pid=27577)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(func pid=27577)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 79 ERROR | 1 RUNNING\n",
            "Current time: 2025-11-01 16:28:12. Total running time: 47min 36s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00079   RUNNING      64     32   0.01               16 |\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "74 more ERROR\n",
            "\u001b[36m(func pid=27577)\u001b[0m [1,  2000] loss: 2.298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-01 16:28:27,026\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_1c521_00079\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1026, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=27577, ip=172.28.0.12, actor_id=7b4b5f372561d9b03fd808b901000000, repr=func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 44, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/function_trainable.py\", line 249, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-1632616050.py\", line 81, in train_cifar\n",
            "AttributeError: module 'ray.tune' has no attribute 'checkpoint_dir'\n",
            "2025-11-01 16:28:27,063\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_cifar_2025-11-01_15-40-36' in 0.0322s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_1c521_00079 errored after 0 iterations at 2025-11-01 16:28:27. Total running time: 47min 50s\n",
            "Error file: /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00079_79_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt\n",
            "\n",
            "Trial status: 80 ERROR\n",
            "Current time: 2025-11-01 16:28:27. Total running time: 47min 50s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2       lr     batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00000   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00001   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00002   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00003   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00004   ERROR        32     16   0.01               16 |\n",
            "| train_cifar_1c521_00005   ERROR        64     16   0.01               16 |\n",
            "| train_cifar_1c521_00006   ERROR        32     32   0.01               16 |\n",
            "| train_cifar_1c521_00007   ERROR        64     32   0.01               16 |\n",
            "| train_cifar_1c521_00008   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00009   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00010   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00011   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00012   ERROR        32     16   0.01               16 |\n",
            "| train_cifar_1c521_00013   ERROR        64     16   0.01               16 |\n",
            "| train_cifar_1c521_00014   ERROR        32     32   0.01               16 |\n",
            "| train_cifar_1c521_00015   ERROR        64     32   0.01               16 |\n",
            "| train_cifar_1c521_00016   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00017   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00018   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00019   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00020   ERROR        32     16   0.01               16 |\n",
            "| train_cifar_1c521_00021   ERROR        64     16   0.01               16 |\n",
            "| train_cifar_1c521_00022   ERROR        32     32   0.01               16 |\n",
            "| train_cifar_1c521_00023   ERROR        64     32   0.01               16 |\n",
            "| train_cifar_1c521_00024   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00025   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00026   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00027   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00028   ERROR        32     16   0.01               16 |\n",
            "| train_cifar_1c521_00029   ERROR        64     16   0.01               16 |\n",
            "| train_cifar_1c521_00030   ERROR        32     32   0.01               16 |\n",
            "| train_cifar_1c521_00031   ERROR        64     32   0.01               16 |\n",
            "| train_cifar_1c521_00032   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00033   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00034   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00035   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00036   ERROR        32     16   0.01               16 |\n",
            "| train_cifar_1c521_00037   ERROR        64     16   0.01               16 |\n",
            "| train_cifar_1c521_00038   ERROR        32     32   0.01               16 |\n",
            "| train_cifar_1c521_00039   ERROR        64     32   0.01               16 |\n",
            "| train_cifar_1c521_00040   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00041   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00042   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00043   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00044   ERROR        32     16   0.01               16 |\n",
            "| train_cifar_1c521_00045   ERROR        64     16   0.01               16 |\n",
            "| train_cifar_1c521_00046   ERROR        32     32   0.01               16 |\n",
            "| train_cifar_1c521_00047   ERROR        64     32   0.01               16 |\n",
            "| train_cifar_1c521_00048   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00049   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00050   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00051   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00052   ERROR        32     16   0.01               16 |\n",
            "| train_cifar_1c521_00053   ERROR        64     16   0.01               16 |\n",
            "| train_cifar_1c521_00054   ERROR        32     32   0.01               16 |\n",
            "| train_cifar_1c521_00055   ERROR        64     32   0.01               16 |\n",
            "| train_cifar_1c521_00056   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00057   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00058   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00059   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00060   ERROR        32     16   0.01               16 |\n",
            "| train_cifar_1c521_00061   ERROR        64     16   0.01               16 |\n",
            "| train_cifar_1c521_00062   ERROR        32     32   0.01               16 |\n",
            "| train_cifar_1c521_00063   ERROR        64     32   0.01               16 |\n",
            "| train_cifar_1c521_00064   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00065   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00066   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00067   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00068   ERROR        32     16   0.01               16 |\n",
            "| train_cifar_1c521_00069   ERROR        64     16   0.01               16 |\n",
            "| train_cifar_1c521_00070   ERROR        32     32   0.01               16 |\n",
            "| train_cifar_1c521_00071   ERROR        64     32   0.01               16 |\n",
            "| train_cifar_1c521_00072   ERROR        32     16   0.0001             16 |\n",
            "| train_cifar_1c521_00073   ERROR        64     16   0.0001             16 |\n",
            "| train_cifar_1c521_00074   ERROR        32     32   0.0001             16 |\n",
            "| train_cifar_1c521_00075   ERROR        64     32   0.0001             16 |\n",
            "| train_cifar_1c521_00076   ERROR        32     16   0.01               16 |\n",
            "| train_cifar_1c521_00077   ERROR        64     16   0.01               16 |\n",
            "| train_cifar_1c521_00078   ERROR        32     32   0.01               16 |\n",
            "| train_cifar_1c521_00079   ERROR        64     32   0.01               16 |\n",
            "+--------------------------------------------------------------------------+\n",
            "\n",
            "Number of errored trials: 80\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                  # failures   error file                                                                                                                                                                                                                  |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_1c521_00000              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00000_0_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt  |\n",
            "| train_cifar_1c521_00001              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00001_1_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt  |\n",
            "| train_cifar_1c521_00002              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00002_2_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt  |\n",
            "| train_cifar_1c521_00003              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00003_3_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt  |\n",
            "| train_cifar_1c521_00004              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00004_4_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt  |\n",
            "| train_cifar_1c521_00005              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00005_5_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt  |\n",
            "| train_cifar_1c521_00006              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00006_6_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-36/error.txt  |\n",
            "| train_cifar_1c521_00007              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00007_7_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-36/error.txt  |\n",
            "| train_cifar_1c521_00008              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00008_8_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt  |\n",
            "| train_cifar_1c521_00009              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00009_9_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt  |\n",
            "| train_cifar_1c521_00010              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00010_10_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00011              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00011_11_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00012              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00012_12_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00013              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00013_13_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00014              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00014_14_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00015              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00015_15_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00016              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00016_16_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00017              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00017_17_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00018              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00018_18_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00019              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00019_19_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00020              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00020_20_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00021              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00021_21_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00022              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00022_22_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00023              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00023_23_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00024              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00024_24_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00025              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00025_25_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00026              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00026_26_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00027              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00027_27_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00028              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00028_28_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00029              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00029_29_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-36/error.txt |\n",
            "| train_cifar_1c521_00030              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00030_30_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00031              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00031_31_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00032              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00032_32_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00033              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00033_33_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00034              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00034_34_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00035              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00035_35_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00036              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00036_36_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00037              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00037_37_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00038              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00038_38_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00039              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00039_39_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00040              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00040_40_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00041              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00041_41_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00042              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00042_42_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00043              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00043_43_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00044              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00044_44_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00045              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00045_45_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00046              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00046_46_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00047              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00047_47_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00048              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00048_48_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00049              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00049_49_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00050              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00050_50_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00051              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00051_51_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00052              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00052_52_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00053              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00053_53_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00054              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00054_54_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00055              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00055_55_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00056              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00056_56_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00057              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00057_57_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00058              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00058_58_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00059              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00059_59_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00060              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00060_60_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00061              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00061_61_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00062              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00062_62_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00063              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00063_63_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00064              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00064_64_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00065              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00065_65_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00066              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00066_66_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00067              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00067_67_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00068              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00068_68_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00069              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00069_69_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00070              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00070_70_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00071              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00071_71_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00072              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00072_72_batch_size=16,l1=32,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00073              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00073_73_batch_size=16,l1=64,l2=16,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00074              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00074_74_batch_size=16,l1=32,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00075              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00075_75_batch_size=16,l1=64,l2=32,lr=0.0001_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00076              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00076_76_batch_size=16,l1=32,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00077              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00077_77_batch_size=16,l1=64,l2=16,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00078              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00078_78_batch_size=16,l1=32,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "| train_cifar_1c521_00079              1   /tmp/ray/session_2025-11-01_15-40-23_394494_187/artifacts/2025-11-01_15-40-36/train_cifar_2025-11-01_15-40-36/driver_artifacts/train_cifar_1c521_00079_79_batch_size=16,l1=64,l2=32,lr=0.0100_2025-11-01_15-40-37/error.txt |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TuneError",
          "evalue": "('Trials did not complete', [train_cifar_1c521_00000, train_cifar_1c521_00001, train_cifar_1c521_00002, train_cifar_1c521_00003, train_cifar_1c521_00004, train_cifar_1c521_00005, train_cifar_1c521_00006, train_cifar_1c521_00007, train_cifar_1c521_00008, train_cifar_1c521_00009, train_cifar_1c521_00010, train_cifar_1c521_00011, train_cifar_1c521_00012, train_cifar_1c521_00013, train_cifar_1c521_00014, train_cifar_1c521_00015, train_cifar_1c521_00016, train_cifar_1c521_00017, train_cifar_1c521_00018, train_cifar_1c521_00019, train_cifar_1c521_00020, train_cifar_1c521_00021, train_cifar_1c521_00022, train_cifar_1c521_00023, train_cifar_1c521_00024, train_cifar_1c521_00025, train_cifar_1c521_00026, train_cifar_1c521_00027, train_cifar_1c521_00028, train_cifar_1c521_00029, train_cifar_1c521_00030, train_cifar_1c521_00031, train_cifar_1c521_00032, train_cifar_1c521_00033, train_cifar_1c521_00034, train_cifar_1c521_00035, train_cifar_1c521_00036, train_cifar_1c521_00037, train_cifar_1c521_00038, train_cifar_1c521_00039, train_cifar_1c521_00040, train_cifar_1c521_00041, train_cifar_1c521_00042, train_cifar_1c521_00043, train_cifar_1c521_00044, train_cifar_1c521_00045, train_cifar_1c521_00046, train_cifar_1c521_00047, train_cifar_1c521_00048, train_cifar_1c521_00049, train_cifar_1c521_00050, train_cifar_1c521_00051, train_cifar_1c521_00052, train_cifar_1c521_00053, train_cifar_1c521_00054, train_cifar_1c521_00055, train_cifar_1c521_00056, train_cifar_1c521_00057, train_cifar_1c521_00058, train_cifar_1c521_00059, train_cifar_1c521_00060, train_cifar_1c521_00061, train_cifar_1c521_00062, train_cifar_1c521_00063, train_cifar_1c521_00064, train_cifar_1c521_00065, train_cifar_1c521_00066, train_cifar_1c521_00067, train_cifar_1c521_00068, train_cifar_1c521_00069, train_cifar_1c521_00070, train_cifar_1c521_00071, train_cifar_1c521_00072, train_cifar_1c521_00073, train_cifar_1c521_00074, train_cifar_1c521_00075, train_cifar_1c521_00076, train_cifar_1c521_00077, train_cifar_1c521_00078, train_cifar_1c521_00079])",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3601163190.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         progress_reporter=reporter)\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3601163190.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, gpus_per_trial)\u001b[0m\n\u001b[1;32m     15\u001b[0m     reporter = CLIReporter(\n\u001b[1;32m     16\u001b[0m         metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n\u001b[0;32m---> 17\u001b[0;31m     result = tune.run(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cifar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mresources_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexperiment_interrupted_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_cifar_1c521_00000, train_cifar_1c521_00001, train_cifar_1c521_00002, train_cifar_1c521_00003, train_cifar_1c521_00004, train_cifar_1c521_00005, train_cifar_1c521_00006, train_cifar_1c521_00007, train_cifar_1c521_00008, train_cifar_1c521_00009, train_cifar_1c521_00010, train_cifar_1c521_00011, train_cifar_1c521_00012, train_cifar_1c521_00013, train_cifar_1c521_00014, train_cifar_1c521_00015, train_cifar_1c521_00016, train_cifar_1c521_00017, train_cifar_1c521_00018, train_cifar_1c521_00019, train_cifar_1c521_00020, train_cifar_1c521_00021, train_cifar_1c521_00022, train_cifar_1c521_00023, train_cifar_1c521_00024, train_cifar_1c521_00025, train_cifar_1c521_00026, train_cifar_1c521_00027, train_cifar_1c521_00028, train_cifar_1c521_00029, train_cifar_1c521_00030, train_cifar_1c521_00031, train_cifar_1c521_00032, train_cifar_1c521_00033, train_cifar_1c521_00034, train_cifar_1c521_00035, train_cifar_1c521_00036, train_cifar_1c521_00037, train_cifar_1c521_00038, train_cifar_1c521_00039, train_cifar_1c521_00040, train_cifar_1c521_00041, train_cifar_1c521_00042, train_cifar_1c521_00043, train_cifar_1c521_00044, train_cifar_1c521_00045, train_cifar_1c521_00046, train_cifar_1c521_00047, train_cifar_1c521_00048, train_cifar_1c521_00049, train_cifar_1c521_00050, train_cifar_1c521_00051, train_cifar_1c521_00052, train_cifar_1c521_00053, train_cifar_1c521_00054, train_cifar_1c521_00055, train_cifar_1c521_00056, train_cifar_1c521_000..."
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pDyf2Ku1H1ba",
        "outputId": "bb86260e-288a-486e-c921-a870dcea604f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cài đặt thử viện skorch"
      ],
      "metadata": {
        "id": "rc-qkO2kBZ83"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "!pip install skorch\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from torch import nn\n",
        "\n",
        "from skorch import NeuralNetClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting skorch\n",
            "  Downloading skorch-1.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from skorch) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from skorch) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from skorch) (1.16.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from skorch) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->skorch) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->skorch) (3.6.0)\n",
            "Downloading skorch-1.2.0-py3-none-any.whl (263 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/263.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.1/263.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: skorch\n",
            "Successfully installed skorch-1.2.0\n"
          ]
        }
      ],
      "metadata": {
        "id": "SVnRxqZpJXwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c02ab6f-4e37-44f6-96c4-d89a190448e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sử dụng thư viện skorch để huấn luyện"
      ],
      "metadata": {
        "id": "Dks1s3-XBZ83"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "trainset, testset = load_data('./data')\n",
        "(X, y) = np.asarray(trainset.data[:]), np.asarray(trainset.targets[:])\n",
        "X = X.reshape((-1, 3, 32, 32))\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.int64)\n",
        "\n",
        "net = NeuralNetClassifier(\n",
        "    Net,\n",
        "    max_epochs=5,\n",
        "    lr=0.01,\n",
        "    # Shuffle training data on each epoch\n",
        "    iterator_train__shuffle=True,\n",
        ")\n",
        "\n",
        "# training with default config\n",
        "net.fit(X, y)\n",
        "y_proba = net.predict_proba(X)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_acc    valid_loss      dur\n",
            "-------  ------------  -----------  ------------  -------\n",
            "      1        \u001b[36m2.3807\u001b[0m       \u001b[32m0.1220\u001b[0m        \u001b[35m2.2917\u001b[0m  10.9420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      2        \u001b[36m2.2095\u001b[0m       \u001b[32m0.1646\u001b[0m        2.4066  7.6358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      3        \u001b[36m2.1264\u001b[0m       \u001b[32m0.2393\u001b[0m        \u001b[35m2.0912\u001b[0m  7.9478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m2.0535\u001b[0m       \u001b[32m0.2644\u001b[0m        \u001b[35m2.0010\u001b[0m  6.9856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m1.9945\u001b[0m       \u001b[32m0.2765\u001b[0m        \u001b[35m1.9543\u001b[0m  7.9418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        }
      ],
      "metadata": {
        "id": "We2jqjXx4tf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2e38153-31af-4e69-9568-e278857ff09e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dùng skorch để lựa chọn siêu tham số"
      ],
      "metadata": {
        "id": "5U5mPRuFBZ83"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# hyperparameter tuning\n",
        "net.set_params(train_split=False, verbose=0)\n",
        "# Liệt kê các giá trị siêu tham số cần search và dùng hàm gridsearchCV để tìm giá trị tối ưu\n",
        "\n",
        "######################\n",
        "### YOUR CODE HERE ###\n",
        "params = {\n",
        "    'lr': [1e-4, 1e-2],\n",
        "    'module__l1': [32, 64],\n",
        "    'module__l2': [16, 32],\n",
        "}\n",
        "gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy', verbose=2)\n",
        "######################\n",
        "\n",
        "gs.fit(X, y)\n",
        "print(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ............lr=0.0001, module__l1=32, module__l2=16; total time=  31.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ............lr=0.0001, module__l1=32, module__l2=16; total time=  30.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ............lr=0.0001, module__l1=32, module__l2=16; total time=  31.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ............lr=0.0001, module__l1=32, module__l2=32; total time=  31.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ............lr=0.0001, module__l1=32, module__l2=32; total time=  30.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ............lr=0.0001, module__l1=32, module__l2=32; total time=  29.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ............lr=0.0001, module__l1=64, module__l2=16; total time=  31.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ............lr=0.0001, module__l1=64, module__l2=16; total time=  31.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ............lr=0.0001, module__l1=64, module__l2=16; total time=  30.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ............lr=0.0001, module__l1=64, module__l2=32; total time=  29.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ............lr=0.0001, module__l1=64, module__l2=32; total time=  30.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ............lr=0.0001, module__l1=64, module__l2=32; total time=  31.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ..............lr=0.01, module__l1=32, module__l2=16; total time=  29.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ..............lr=0.01, module__l1=32, module__l2=16; total time=  29.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ..............lr=0.01, module__l1=32, module__l2=16; total time=  28.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ..............lr=0.01, module__l1=32, module__l2=32; total time=  27.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ..............lr=0.01, module__l1=32, module__l2=32; total time=  28.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ..............lr=0.01, module__l1=32, module__l2=32; total time=  29.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ..............lr=0.01, module__l1=64, module__l2=16; total time=  28.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ..............lr=0.01, module__l1=64, module__l2=16; total time=  28.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ..............lr=0.01, module__l1=64, module__l2=16; total time=  28.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ..............lr=0.01, module__l1=64, module__l2=32; total time=  28.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ..............lr=0.01, module__l1=64, module__l2=32; total time=  27.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ..............lr=0.01, module__l1=64, module__l2=32; total time=  27.7s\n",
            "best score: 0.301, best params: {'lr': 0.01, 'module__l1': 64, 'module__l2': 16}\n"
          ]
        }
      ],
      "metadata": {
        "id": "Y0ixOx3SHd8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd02ead0-d2e2-4d67-9647-124b0b4b89ab"
      }
    }
  ]
}